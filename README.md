
# Neural Network Framework for Regression Tasks

(Readme generated by Deepseek)

A flexible neural network implementation supporting various optimizers, activation functions, and data handling utilities. Designed for regression tasks with features like batch training, data normalization, and model validation.

## Features

- **Data Handling**

  - Load CSV/XLSX datasets.
  - Split data into training, validation, and testing sets.
  - Normalization: Standardization and Min-Max scaling.
  - Batch splitting for mini-batch training.
- **Neural Network**

  - Customizable layers with activation functions: `sigmoid`, `tanh`, `relu`, `leaky_relu`, `linear`, etc.
  - Weight initialization: `He`, `Xavier`, and normal variants.
  - Loss function: Mean Squared Error (MSE).
- **Optimizers**

  - Stochastic Gradient Descent (SGD)
  - Polyak Momentum
  - RMSProp
  - Adam
- **Training & Evaluation**

  - Batched or full-batch training.
  - Validation loss tracking.
  - Interactive testing mode for predictions.

## Installation

1. **Dependencies**
   Ensure the following packages are installed:

   ```bash
   pip install numpy pandas
   ```
2. **Clone the Repository**

   ```
   git clone https://github.com/your-username/neural-network-regression.git cd neural-network-regression
   ```


## Usage

### 1. Prepare Dataset

* Format: CSV or XLSX file with features and a label column (default column name: `label`; customizable in `split()`).
* Example CSV structure for `LinearRegression.csv`:
* ```
  x1,x2,out
  1,2,3
  3,4,7
  ...
  ```

### 2. Configure Model

Modify `Main.py` to set up your model:

**python**

```
# Define dataset and split ratio
dataset = Dataset(path="Data/LinearRegression.csv", split_ratio=Ratio(0.7, 0.2, 0.1))
dataset.load()
dataset.split(label_column="out")  # Match your label column name
dataset.normalize(normalize_labels=True)

# Define network architecture (input size inferred from data)
layers = [
    Layer(num_of_units=1, activation_mode="linear", initialization_mode="He")
]
nn = NeuralNetwork(layers=layers, loss_func="MSE", dataset=dataset)

# Choose optimizer (options: SGD(), RMSProp(), Adam())
optimizer = PolyakMomentum()
trainer = Trainer(nn=nn, optimizer=optimizer, epochs=10000, learning_rate=0.01)
```

### 3. Train the Model

**python**

```
trainer.train(batched=True, batch_size=32, shuffle=True)
```

### 4. Test the Model

After training, the interactive mode will prompt for input features:

```
x1: 5
x2: 3
Out: 8.12
```

## File Structure

| File                  | Description                                                             |
| --------------------- | ----------------------------------------------------------------------- |
| `DataHandeling.py`  | Data loading, splitting, normalization, and batching.                   |
| `Neural_Network.py` | Neural network and layer implementation (forward/backward propagation). |
| `Trainers.py`       | Optimizers (SGD, Momentum, RMSProp, Adam) and training loop.            |
| `Util.py`           | Activation functions, cost calculations, and normalization utilities.   |
| `Main.py`           | Example usage: dataset setup, model training, and interactive testing.  |

## Configuration

### Data Normalization

* **Modes** : `standardization` (default) or `minmax`.
* **Labels** : Set `normalize_labels=True` in `dataset.normalize()` if labels require scaling.
  ⚠️ Denormalization (in testing) only supports `standardization`.

### Layer Configuration

* **Initialization** : Choose `He`, `Xavier`, `HeNormal`, or `XavierNormal`.
* **Activation** : Options include `linear` (for regression output), `relu`, `sigmoid`, etc.

## Example

**python**

```
# Define a deeper network
layers = [
    Layer(num_of_units=64, activation_mode="relu", initialization_mode="HeNormal"),
    Layer(num_of_units=32, activation_mode="tanh", initialization_mode="Xavier"),
    Layer(num_of_units=1, activation_mode="linear")
]

# Use Adam optimizer
optimizer = Adam(beta1=0.9, beta2=0.999)
trainer = Trainer(nn=nn, optimizer=optimizer, epochs=5000, learning_rate=0.001)
```

## Contributing

Contributions are welcome! Open an issue or submit a pull request for improvements or bug fixes.
